{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbrazonath/TP_AP/blob/main/BRAZON-JOSMAR-DL-TP2-Co22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad de Buenos Aires\n",
        "# Aprendizaje Profundo - TP2\n",
        "# Cohorte 22 - 5to bimestre 2025"
      ],
      "metadata": {
        "id": "ncx7zZsK4xu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El segundo TP comienza la semana de la clase 4 y la ventana de entrega estará abierta hasta las **23hs del viernes 28 de noviembre (hora de Argentina)**. La resolución del TP es **individual**. Pueden utilizar tanto los contenidos vistos en clase, como otra bibliografía externa. Si se toman ideas de fuentes externas deben ser correctamente citadas incluyendo el correspondiente link o página de libro.\n",
        "\n",
        "ESTE TP2 EQUIVALE A UN TERCIO DE SU NOTA FINAL.\n",
        "\n",
        "El formato de entrega debe ser un link a un notebook de google colab. Importante permitir acceso a gvilcamiza.ext@fi.uba.ar y **habilitar los comentarios, para poder darles el feedback**. Si no lo hacen así no se podrá dar el feedback respectivo por cada pregunta.\n",
        "\n",
        "El envío **se realizará en el siguiente link de google forms: [link](https://forms.gle/kvD11QBVSHbHA8QDA)**. Tanto los resultados, gráficas, como el código y las explicaciones deben quedar guardados y visualizables en el colab.\n",
        "\n",
        "**NO SE VALIDARÁN ENVÍOS POR CORREO, EL MÉTODO DE ENTREGA ES SOLO POR EL FORMS.**\n",
        "\n",
        "**Consideraciones a tener en cuenta:**\n",
        "- Se entregará 1 solo colab para este TP2.\n",
        "- Renombrar el archivo de la siguiente manera: **APELLIDO-NOMBRE-DL-TP2-Co22.ipynb**\n",
        "- Los códigos deben poder ejecutarse.\n",
        "- **IMPORTANTE:** Los resultados, cómo el código, los gráficos, los prints y las explicaciones deben quedar guardados y visualizables en el mismo notebook.\n",
        "- **Prestar mucha atención a cada consigna, responder las preguntas justo debajo del enunciado que corresponda.**\n",
        "- Solo se revisarán los trabajos que hayan sido enviados por el forms."
      ],
      "metadata": {
        "id": "WPw0M49K43vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CASO: Adult Census Income"
      ],
      "metadata": {
        "id": "s9k02PRp5XGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo del trabajo es construir un modelo de clasificación binaria que, a partir de los datos censales de diferentes hogares, determine si un individuo pertenece al grupo de mayores o de menores ingresos. Para ello, se empleará un conjunto de variables demográficas, laborales y socioeconómicas que describen las características de cada persona. El estudio debe incluir el análisis exploratorio del dataset, la selección y justificación de las transformaciones más adecuadas para cada variable, la construcción de modelos basados tanto en técnicas de codificación tradicionales como también en representaciones avanzadas mediante embeddings, y la comparación final del desempeño obtenido por cada enfoque.\n",
        "\n",
        "**Para este caso de estudio, consideraremos como variable de alta cardinalidad a las que tengan 10 o más valores únicos.**\n"
      ],
      "metadata": {
        "id": "cDCR7S3W8R8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontrarán el dataset en el siguiente enlace de drive: [link](https://drive.google.com/drive/folders/1S-usUXkJP6OdzUS0zdC5CW-XqegiXzln?usp=sharing)\n",
        "\n",
        "Está compuesto por los siguientes features:\n",
        "- **age**: Edad del individuo expresada en años.\n",
        "\n",
        "- **workclass**: Tipo de empleador o relación laboral del individuo. Describe si trabaja en el sector privado, gobierno estatal, local, federal, por cuenta propia, sin remuneración, etc.\n",
        "\n",
        "- **education**: Nivel educativo alcanzado. Incluye categorías como secundaria completada, licenciatura, maestría, doctorado, etc.\n",
        "\n",
        "- **marital-status**: Estado civil (casado, nunca casado, divorciado, viudo, etc.).\n",
        "\n",
        "- **occupation**: Tipo de ocupación o área laboral, donde se incluye ventas, servicios de protección, técnicos, gerencia ejecutiva, fuerzas armadas, etc.\n",
        "\n",
        "- **relationship**: Relación del individuo con el jefe del hogar como esposo, esposa, hijo propio, pariente, no familiar, etc.\n",
        "\n",
        "- **race**: Autoidentificación racial como blanca, negra, indígena, asiática, isleños del Pacífico, entre otras.\n",
        "\n",
        "- **sex**: Sexo biológico del individuo (masculino o femenino).\n",
        "\n",
        "- **capital-gain**: Ingresos obtenidos por ganancia de capital (por ejemplo, venta de acciones o propiedades).\n",
        "\n",
        "- **capital-loss**: Pérdidas declaradas por capital.\n",
        "\n",
        "- **hours-per-week**: Cantidad de horas trabajadas por semana.\n",
        "\n",
        "- **native-country**: País de nacimiento del individuo. Incluye Estados Unidos y una lista amplia de países del mundo.\n",
        "\n",
        "- **income (target)**: Clasificación binaria que indica si el ingreso anual del individuo es mejor o igual a 50K o mayor a 50K."
      ],
      "metadata": {
        "id": "JVRXhrUJQHDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Análisis exploratorio de los datasets (2 puntos)\n",
        "\n",
        "- Realizar un EDA apoyado en gráficas adecuadas y coherentes para el caso de estudio.\n",
        "- Analizar detalladamente los valores únicos de cada variable categórica e identificar su nivel de cardinalidad.\n",
        "- Justificar de manera detallada el tipo de transformación que se le asignará a cada variable, en especial a las categóricas. **Dependiendo de su cardinalidad, su contexto y/o lógica interna de orden**, podrán transformarse mediante label/ordinal encoding, one-hot encoding o mediante una capa de embeddings dentro del modelo.\n",
        "- No es necesario aplicar la misma transformación para todas las variables categóricas. El dataset puede (y debe) incluir diferentes tipos de transformaciones según las características de cada variable.\n",
        "- Redactar explícitamente la decisión final adoptada para cada variable y su justificación correspondiente."
      ],
      "metadata": {
        "id": "HTFFhO03eyaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "\n",
        "gdown.download(f\"https://drive.google.com/uc?id=16IemDxpuAsmlof9BnZGDPXHCSSocNYDF\", \"adult_train.csv\", quiet=False)\n",
        "gdown.download(f\"https://drive.google.com/uc?id=1hxyl8v_AQGjxPsiNhcO5n1jQ2NVHbU9f\", \"adult_val.csv\", quiet=False)\n",
        "\n",
        "df_train = pd.read_csv(\"adult_train.csv\")\n",
        "df_val   = pd.read_csv(\"adult_val.csv\")\n",
        "\n",
        "df_train.head(), df_val.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "STwjCxDHGpNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis de nulos:"
      ],
      "metadata": {
        "id": "_nQExDd-HmaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def ver_nulos(df):\n",
        "    # nulos reales\n",
        "    nulos = df.isna().sum()\n",
        "    nulos = nulos[nulos > 0]\n",
        "\n",
        "    # valores \"?\"\n",
        "    preguntas = (df == \"?\").sum()\n",
        "    preguntas = preguntas[preguntas > 0]\n",
        "\n",
        "    # strings vacíos\n",
        "    vacios = (df == \"\").sum()\n",
        "    vacios = vacios[vacios > 0]\n",
        "\n",
        "    # strings con solo espacios (solo columnas de texto)\n",
        "    obj = df.select_dtypes(include=\"object\")\n",
        "    espacios = obj.apply(lambda col: col.str.strip().eq(\"\").sum())\n",
        "    espacios = espacios[espacios > 0]\n",
        "\n",
        "    print(\"Nulos estándar (>0):\")\n",
        "    display(nulos)\n",
        "\n",
        "    print(\"\\nValores '?' (>0):\")\n",
        "    display(preguntas)\n",
        "\n",
        "    print(\"\\nStrings vacíos '' (>0):\")\n",
        "    display(vacios)\n",
        "\n",
        "    print(\"\\nStrings con espacios (>0):\")\n",
        "    display(espacios)\n"
      ],
      "metadata": {
        "id": "ncJ9TBazHn3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ver_nulos(df_train)\n",
        "ver_nulos(df_val)\n"
      ],
      "metadata": {
        "id": "LTcljHd8Hp9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['is_married'] = df_train['marital-status'].str.contains('Married').astype(int)\n",
        "df_val['is_married']   = df_val['marital-status'].str.contains('Married').astype(int)\n",
        "\n",
        "df_train['is_male'] = (df_train['sex'] == 'Male').astype(int)\n",
        "df_val['is_male']   = (df_val['sex'] == 'Male').astype(int)\n",
        "\n",
        "df_train['has_capital_gain'] = (df_train['capital-gain'] > 0).astype(int)\n",
        "df_val['has_capital_gain']   = (df_val['capital-gain'] > 0).astype(int)\n",
        "\n",
        "df_train['has_capital_loss'] = (df_train['capital-loss'] > 0).astype(int)\n",
        "df_val['has_capital_loss']   = (df_val['capital-loss'] > 0).astype(int)\n",
        "\n",
        "df_train['is_government_worker'] = df_train['workclass'].apply(lambda x: 1 if 'gov' in str(x).lower() else 0)\n",
        "df_val['is_government_worker']   = df_val['workclass'].apply(lambda x: 1 if 'gov' in str(x).lower() else 0)\n"
      ],
      "metadata": {
        "id": "DQsfJRR6Inzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No hay nulos, pasamos a outliers"
      ],
      "metadata": {
        "id": "ZRMrnn8YICyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = df_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "cat_vars = df_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "num_vars, cat_vars\n"
      ],
      "metadata": {
        "id": "LbsaJ_jaJXi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El análisis de valores atípicos se realizó únicamente sobre las variables numéricas continuas (age, capital-gain, capital-loss, hours-per-week). Las variables derivadas binarias (is_married, is_male, has_capital_gain, has_capital_loss, is_government_worker) no fueron sometidas a detección de outliers, ya que solo toman valores {0,1} y en este tipo de codificación no existe el concepto de valor extremo: cualquier valor distinto de 0 o 1 sería directamente un error de carga, no un outlier estadístico."
      ],
      "metadata": {
        "id": "gc3OKMtVK6h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "def contar_outliers_IQR(df, cols):\n",
        "    outliers = {}\n",
        "    for col in cols:\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        li = q1 - 1.5 * iqr\n",
        "        ls = q3 + 1.5 * iqr\n",
        "        outliers[col] = ((df[col] < li) | (df[col] > ls)).sum()\n",
        "    return pd.Series(outliers)\n",
        "\n",
        "outliers_train = contar_outliers_IQR(df_train, num_cols)\n",
        "outliers_train\n"
      ],
      "metadata": {
        "id": "LhtoG9aEIFZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def porcentaje_outliers_IQR(df, cols):\n",
        "    n = len(df)\n",
        "    porcentajes = {}\n",
        "    for col in cols:\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        li = q1 - 1.5 * iqr\n",
        "        ls = q3 + 1.5 * iqr\n",
        "        porcentajes[col] = ((df[col] < li) | (df[col] > ls)).sum() * 100 / n\n",
        "    return pd.Series(porcentajes)\n",
        "\n",
        "porc_outliers_train = porcentaje_outliers_IQR(df_train, num_cols)\n",
        "porc_outliers_train\n"
      ],
      "metadata": {
        "id": "3JvBDurZGXWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "num_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(1, 4, i)\n",
        "    sns.boxplot(y=df_train[col])\n",
        "    plt.title(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6rhUwpHELIOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratamiento de outliers en variables numéricas\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1) Aplicar log a capital-gain y capital-loss\n",
        "# En este caso apliqué log1 en estas dos variables ya que son variables muy desparejas, con muchos ceros y algunos montos.\n",
        "#En vez de eliminarlos (porque son datos reales), aplico log para bajar la asimetría y hacer que esos valores extremos no distorsionen el aprendizaje.\n",
        "df_train['capital-gain'] = np.log1p(df_train['capital-gain'])\n",
        "df_train['capital-loss'] = np.log1p(df_train['capital-loss'])\n",
        "\n",
        "df_val['capital-gain'] = np.log1p(df_val['capital-gain'])\n",
        "df_val['capital-loss'] = np.log1p(df_val['capital-loss'])\n",
        "\n",
        "# 2) Escalar age y hours-per-week\n",
        "#En este caso para estas dos decidí aplicar minMax ya que son variables continuas.\n",
        "cols_to_scale = ['age', 'hours-per-week']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "df_train[cols_to_scale] = scaler.fit_transform(df_train[cols_to_scale])\n",
        "df_val[cols_to_scale]   = scaler.transform(df_val[cols_to_scale])\n"
      ],
      "metadata": {
        "id": "-EjZ6QJWLcm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veo cómo viene mi dataset"
      ],
      "metadata": {
        "id": "iq5zWi7WNfL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns.tolist()\n"
      ],
      "metadata": {
        "id": "i47F2vnrNUwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decido eliminar sex ya que esta informacion la tengo en is_male\n"
      ],
      "metadata": {
        "id": "XThPzi3WhEjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(columns=['sex'])\n",
        "df_val   = df_val.drop(columns=['sex'])\n"
      ],
      "metadata": {
        "id": "TaGzp5TaN_6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificar variables categóricas\n",
        "cat_cols = df_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Contar valores únicos\n",
        "unique_counts = df_train[cat_cols].nunique()\n",
        "\n",
        "# Separar en bajas y altas según el enunciado\n",
        "low_cardinality  = unique_counts[unique_counts < 10].index.tolist()\n",
        "high_cardinality = unique_counts[unique_counts >= 10].index.tolist()\n",
        "\n",
        "low_cardinality, high_cardinality\n"
      ],
      "metadata": {
        "id": "aILtEyH2Psdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entonces hasta ahora tenemos\n",
        "\n",
        "\n",
        "['workclass', 'marital-status', 'relationship', 'race', 'income'] -- Variables de baja cardinalidad\n",
        "\n",
        " ['education', 'occupation', 'native-country'] -- Variables de alta cardinalidad (más de 10 valores únicos cómo lo definen en el enunciado)\n"
      ],
      "metadata": {
        "id": "K0NWjkrGQAk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver cuántas variables tienen, para definir el tratamiento."
      ],
      "metadata": {
        "id": "4ro0byGxSGyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables categóricas\n",
        "cat_cols = df_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Conteo de valores únicos\n",
        "unique_counts = df_train[cat_cols].nunique().sort_values()\n",
        "\n",
        "unique_counts"
      ],
      "metadata": {
        "id": "EgiTJm16SJ2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1) Codificación del target\n",
        "df_train['income'] = df_train['income'].map({'>50K': 1, '<=50K': 0})\n",
        "df_val['income']   = df_val['income'].map({'>50K': 1, '<=50K': 0})\n",
        "\n",
        "# 2) One-Hot Encoding (baja cardinalidad)\n",
        "low_cardinality = ['race', 'relationship', 'workclass', 'marital-status']\n",
        "\n",
        "df_train = pd.get_dummies(df_train, columns=low_cardinality, drop_first=True)\n",
        "df_val   = pd.get_dummies(df_val,   columns=low_cardinality, drop_first=True)\n",
        "\n",
        "# Alinear columnas entre train y val\n",
        "df_train, df_val = df_train.align(df_val, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# 3) Label Encoding (embeddings)\n",
        "high_cardinality = ['occupation', 'education', 'native-country']\n",
        "\n",
        "label_encoders = {}\n",
        "\n",
        "for col in high_cardinality:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    df_val[col]   = le.transform(df_val[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "df_train.head()\n"
      ],
      "metadata": {
        "id": "Bsmij--fXZYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separé las variables categóricas según su cantidad de valores únicos.\n",
        "Para las de baja cardinalidad usé One-Hot Encoding, porque tienen pocos valores y no generan demasiadas columnas extra.\n",
        "\n",
        "En cambio, para las de alta cardinalidad elegí Label Encoding porque en la segunda parte del TP vamos a trabajar con embeddings. El modelo necesita que esas categorías estén codificadas como números enteros para poder aprender una representación interna (el embedding). Por eso no aplico OHE ahí: no tendría sentido generar decenas de columnas cuando el objetivo es que el embedding aprenda una representación compacta.\n",
        "\n",
        "La variable target (income) también era categórica, pero como se trata de un problema de clasificación binaria, la convertí directamente a 0 y 1, que es el formato que necesita el modelo para entrenar."
      ],
      "metadata": {
        "id": "DNuhh8arkbuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avanzamos con el escalado\n",
        "\n",
        "\n",
        "\n",
        "Aplicaré MinMaxScaler únicamente sobre las variables numéricas continuas porque sus rangos eran muy distintos entre sí (“age”, “capital-gain”, “capital-loss”, “hours-per-week”). No aplicaré escalado sobre variables binarias, dummies ni sobre las que usarán embeddings, ya que esas representaciones no lo requieren."
      ],
      "metadata": {
        "id": "Eu5dpFavhOBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "num_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
        "df_val[num_cols]   = scaler.transform(df_val[num_cols])\n"
      ],
      "metadata": {
        "id": "A7tuLyGRhPs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['income'].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "aHAAhcDGlsVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la variable target income observo un desbalance moderado: alrededor del 75% de los registros pertenecen a la clase 0 (ingresos <=50K) y el 25% a la clase 1 (ingresos >50K).\n",
        "No llego a hacer un rebalanceo del dataset ahora sino que este desbalance lo voy a manejar en la etapa de modelado."
      ],
      "metadata": {
        "id": "rYUVnVASmJQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columnas de entrada (todas menos la target)\n",
        "feature_cols = [c for c in df_train.columns if c != 'income']\n",
        "\n",
        "# Train\n",
        "X_train = df_train[feature_cols].copy()\n",
        "y_train = df_train['income'].astype(int).copy()\n",
        "\n",
        "# Validación\n",
        "X_val = df_val[feature_cols].copy()\n",
        "y_val = df_val['income'].astype(int).copy()\n",
        "\n",
        "X_train.shape, X_val.shape, y_train.value_counts(normalize=True), y_val.value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "Yy7st3eDpUiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Diseño y entrenamiento de un modelo con embeddings (3 puntos)\n",
        "\n",
        "- Implementar las transformaciones definidas en el punto anterior e incorporarlas al flujo de entrenamiento.\n",
        "- El modelo debe incluir, como mínimo, una capa de embedding para representar alguna de las variables categóricas.\n",
        "- La elección de la dimensión del o los embeddings queda a criterio del estudiante, pero debe estar correctamente fundamentada. Recuerden que no es obligatorio que todos los embeddings tengan la misma dimensión.\n",
        "- La configuración arquitectónica (número de capas, neuronas por capa, función de activación) es de libre elección.\n",
        "- Incluir dropout en las capas ocultas de la red.\n",
        "- Utilizar Adam o alguna de sus variantes como optimizador.\n",
        "- Seleccionar la función de costo apropiada entre Binary CrossEntropyLoss o Categorical CrossEntropyLoss, según la formulación del problema.\n",
        "- Mostrar las curvas de accuracy vs epoch y F1 macro vs epoch para los sets de entrenamiento y validación.\n",
        "- Presentar un classification report generado con sklearn.\n",
        "- Presentar una matriz de confusión absoluta y otra normalizada por fila, correspondientes al set de validación."
      ],
      "metadata": {
        "id": "3Bu4qqF_F3Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "id": "cUgFX3GxrWbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Columnas que vamos a representar con embeddings (alta cardinalidad)\n",
        "emb_cols = ['occupation', 'education', 'native-country']\n",
        "\n",
        "# Aseguro que existan y sean categóricas\n",
        "for col in emb_cols:\n",
        "    X_train[col] = X_train[col].astype('category')\n",
        "    X_val[col]   = X_val[col].astype('category')\n",
        "\n",
        "# Construyo códigos consistentes entre train y val\n",
        "emb_sizes = {}          # {col: (n_categorias, dim_embedding)}\n",
        "X_train_emb = X_train[emb_cols].copy()\n",
        "X_val_emb   = X_val[emb_cols].copy()\n",
        "\n",
        "for col in emb_cols:\n",
        "    cats = pd.concat([X_train_emb[col], X_val_emb[col]], axis=0)\n",
        "    codes, uniques = pd.factorize(cats)\n",
        "    n_cats = len(uniques)\n",
        "\n",
        "    # dimensión del embedding: entre 2 y sqrt(n_cats), redondeada\n",
        "    emb_dim = int(np.clip(round(np.sqrt(n_cats)), 2, n_cats))\n",
        "\n",
        "    emb_sizes[col] = (n_cats, emb_dim)\n",
        "\n",
        "    X_train_emb[col] = codes[:len(X_train)]\n",
        "    X_val_emb[col]   = codes[len(X_train):]\n",
        "\n",
        "\n",
        "# Columnas numéricas\n",
        "num_cols = [c for c in X_train.columns if c not in emb_cols]\n",
        "\n",
        "# 1. Instanciamos el Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Ajustamos y transformamos\n",
        "X_train_num_scaled = scaler.fit_transform(X_train[num_cols])\n",
        "X_val_num_scaled   = scaler.transform(X_val[num_cols])\n",
        "\n",
        "# 3. Tensores\n",
        "# Categoricos\n",
        "X_train_cat_t = torch.tensor(X_train_emb[emb_cols].values, dtype=torch.long)\n",
        "X_val_cat_t   = torch.tensor(X_val_emb[emb_cols].values,   dtype=torch.long)\n",
        "\n",
        "# Numéricos\n",
        "X_train_num_t = torch.tensor(X_train_num_scaled, dtype=torch.float32)\n",
        "X_val_num_t   = torch.tensor(X_val_num_scaled,   dtype=torch.float32)\n",
        "\n",
        "# Targets\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_val_t   = torch.tensor(y_val.values,   dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Datasets y dataloaders\n",
        "train_ds = TensorDataset(X_train_cat_t, X_train_num_t, y_train_t)\n",
        "val_ds   = TensorDataset(X_val_cat_t,   X_val_num_t,   y_val_t)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "\n",
        "# Verificamos\n",
        "len(train_ds), len(val_ds), len(num_cols), emb_sizes"
      ],
      "metadata": {
        "id": "GwYKtfJurYwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabularEmbeddingModel(nn.Module):\n",
        "    def __init__(self, emb_sizes, n_numeric, hidden_dims=[64, 32], p_dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # guardo el orden de las columnas categóricas\n",
        "        self.emb_cols = list(emb_sizes.keys())\n",
        "\n",
        "        # capas de embedding\n",
        "        self.emb_layers = nn.ModuleDict({\n",
        "            col: nn.Embedding(num_cats, emb_dim)\n",
        "            for col, (num_cats, emb_dim) in emb_sizes.items()\n",
        "        })\n",
        "\n",
        "        total_emb_dim = sum(emb_dim for _, (_, emb_dim) in emb_sizes.items())\n",
        "        input_dim = total_emb_dim + n_numeric\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(p_dropout))\n",
        "            prev_dim = h\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))  # salida binaria (logit)\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x_cat, x_num):\n",
        "        # x_cat: (batch, n_emb_cols)\n",
        "        emb_outputs = []\n",
        "        for i, col in enumerate(self.emb_cols):\n",
        "            e = self.emb_layers[col](x_cat[:, i])\n",
        "            emb_outputs.append(e)\n",
        "\n",
        "        x = torch.cat(emb_outputs + [x_num], dim=1)\n",
        "        logits = self.mlp(x).squeeze(1)  # (batch,)\n",
        "        return logits\n",
        "\n",
        "n_numeric = len(num_cols)\n",
        "model = TabularEmbeddingModel(emb_sizes, n_numeric).to(device)\n",
        "model\n"
      ],
      "metadata": {
        "id": "2lOIx03PrcZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weight = torch.tensor([3.0]).to(device)\n",
        "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "6jEklFKjLUEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Ponderación para la clase positiva (por el desbalance)\n",
        "pos_weight = torch.tensor([3.0]).to(device)\n",
        "\n",
        "# Función de costo y optimizador\n",
        "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer  = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "\n",
        "# Estructura para guardar el historial\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_acc\": [],\n",
        "    \"train_f1\": [],\n",
        "    \"val_f1\": []\n",
        "}\n",
        "\n",
        "# Número de épocas\n",
        "n_epochs = 30\n",
        "\n",
        "# Targets como arrays de numpy\n",
        "y_train_np = y_train.values\n",
        "y_val_np   = y_val.values\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # --- Modo entrenamiento ---\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for x_cat_b, x_num_b, y_b in train_loader:\n",
        "        x_cat_b = x_cat_b.to(device)\n",
        "        x_num_b = x_num_b.to(device)\n",
        "        y_b     = y_b.to(device).view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x_cat_b, x_num_b)\n",
        "        loss   = criterion(logits, y_b)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # --- MÉTRICAS TRAIN ---\n",
        "    with torch.no_grad():\n",
        "        # Pasamos todo el set de train\n",
        "        train_logits = model(X_train_cat_t.to(device),\n",
        "                             X_train_num_t.to(device))\n",
        "        probs_train  = torch.sigmoid(train_logits).cpu().numpy()\n",
        "        preds_train  = (probs_train >= 0.5).astype(int)\n",
        "\n",
        "    acc_train  = accuracy_score(y_train_np, preds_train)\n",
        "    f1_train   = f1_score(y_train_np, preds_train, average='macro')\n",
        "    loss_train = np.mean(train_losses)\n",
        "\n",
        "    # --- MÉTRICAS VALIDACIÓN ---\n",
        "    with torch.no_grad():\n",
        "        logits_val = model(X_val_cat_t.to(device),\n",
        "                           X_val_num_t.to(device))\n",
        "        probs_val  = torch.sigmoid(logits_val).cpu().numpy()\n",
        "        preds_val  = (probs_val >= 0.5).astype(int)\n",
        "        loss_val   = criterion(\n",
        "            logits_val,\n",
        "            y_val_t.to(device).view(-1)\n",
        "        ).item()\n",
        "\n",
        "    acc_val = accuracy_score(y_val_np, preds_val)\n",
        "    f1_val  = f1_score(y_val_np, preds_val, average='macro')\n",
        "\n",
        "    val_probs = probs_val.copy() # Guardamos para usar después en el umbral\n",
        "\n",
        "    # Guardar historial\n",
        "    history[\"train_loss\"].append(loss_train)\n",
        "    history[\"val_loss\"].append(loss_val)\n",
        "    history[\"train_acc\"].append(acc_train)\n",
        "    history[\"val_acc\"].append(acc_val)\n",
        "    history[\"train_f1\"].append(f1_train)\n",
        "    history[\"val_f1\"].append(f1_val)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} \"\n",
        "        f\"loss_tr: {loss_train:.4f}  loss_val: {loss_val:.4f} \"\n",
        "        f\"acc_tr: {acc_train:.3f}  acc_val: {acc_val:.3f} \"\n",
        "        f\"f1_macro_tr: {f1_train:.3f}  f1_macro_val: {f1_val:.3f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "JThvwvVprhfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Inicializamos variables para guardar el campeón\n",
        "best_thr = 0.5\n",
        "best_f1 = 0.0\n",
        "\n",
        "thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "for thr in thresholds:\n",
        "    # Aplicar umbral temporal\n",
        "    preds = (val_probs >= thr).astype(int)\n",
        "\n",
        "    # Calculamos métrica\n",
        "    current_f1 = f1_score(y_val_np, preds)\n",
        "\n",
        "    # Si el F1 actual es mayor al mejor que teníamos, actualizamos las variables\n",
        "    if current_f1 > best_f1:\n",
        "        best_f1 = current_f1\n",
        "        best_thr = thr\n",
        "\n",
        "    # Imprimir progreso\n",
        "    # print(f\"Umbral: {thr:.2f} -> F1: {current_f1:.4f}\")\n",
        "\n",
        "print(f\"\\n¡Mejor F1 encontrado: {best_f1:.4f} con umbral {best_thr:.2f}!\")\n",
        "print(f\"La variable 'best_thr' ahora vale: {best_thr}\")"
      ],
      "metadata": {
        "id": "f5XEon0DtfaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el dataset está desbalanceado, el umbral por defecto (0.5) no necesariamente maximiza la métrica F1. Por eso, probé diferentes thresholds entre 0.1 y 0.9 y seleccioné aquel que maximizaba el F1 en validación.\n"
      ],
      "metadata": {
        "id": "T7yamP7RvnXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Umbral elegido por mejor F1\n",
        "print(f\"Usando el umbral óptimo calculado: {best_thr:.2f}\")\n",
        "\n",
        "# Aplicar el umbral\n",
        "y_val_pred = (val_probs >= best_thr).astype(int)\n",
        "\n",
        "# Calcular métricas\n",
        "f1_final = f1_score(y_val_np, y_val_pred)\n",
        "\n",
        "print(f\"F1 final en validación con umbral {best_thr}: {f1_final:.4f}\\n\")\n",
        "\n",
        "print(\"Classification report (validación):\\n\")\n",
        "print(classification_report(y_val_np, y_val_pred, digits=4))\n",
        "\n",
        "print(\"Matriz de confusión (validación):\")\n",
        "print(confusion_matrix(y_val_np, y_val_pred))\n",
        "\n",
        "print(\"\\nMatriz de confusión (Normalizada por fila):\")\n",
        "cm_norm = confusion_matrix(y_val_np, y_val_pred, normalize='true')\n",
        "print(cm_norm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=['<=50K', '>50K'])\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "disp.plot(cmap='Blues', values_format='.2f', ax=ax)\n",
        "plt.title(\"Matriz de Confusión Normalizada\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EjytTcwENrcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# uso la cantidad de puntos guardados en history\n",
        "n_points = len(history[\"train_acc\"])\n",
        "epochs   = range(1, n_points + 1)\n",
        "\n",
        "# accuracy\n",
        "plt.figure()\n",
        "plt.plot(epochs, history[\"train_acc\"], label=\"acc_train\")\n",
        "plt.plot(epochs, history[\"val_acc\"],   label=\"acc_val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# F1 macro\n",
        "plt.figure()\n",
        "plt.plot(epochs, history[\"train_f1\"], label=\"f1_train\")\n",
        "plt.plot(epochs, history[\"val_f1\"],   label=\"f1_val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 macro\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wBO9YF04OAHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c) Diseño y entrenamiento de un modelo sin embeddings (3 puntos)\n",
        "\n",
        "- Entrenar un segundo modelo, aplicando one-hot encoding a todas las variables que en el punto b) fueron representadas mediante embeddings.\n",
        "- Mantener exactamente la misma arquitectura del modelo anterior: igual número de capas, mismas neuronas, mismas funciones de activación y la misma probabilidad de dropout.\n",
        "- Presentar las mismas métricas, visualizaciones y reportes que en el modelo con embeddings."
      ],
      "metadata": {
        "id": "Y4QiRQVtMtzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Definir columnas\n",
        "ohe_cols = ['occupation', 'education', 'native-country'] # Las que antes eran embeddings\n",
        "num_cols = [c for c in X_train.columns if c not in ohe_cols] # Las numéricas\n",
        "\n",
        "# 2. Concatenar para asegurar mismas columnas (dummies)\n",
        "# Marcamos dónde termina el train para separar después\n",
        "n_train = len(X_train)\n",
        "\n",
        "# Unimos solo para generar el OHE consistente\n",
        "X_full = pd.concat([X_train, X_val], axis=0)\n",
        "\n",
        "# Aplicamos One-Hot Encoding (drop_first=False para tener representación completa)\n",
        "X_full_ohe = pd.get_dummies(X_full, columns=ohe_cols, dtype=float)\n",
        "\n",
        "# Separamos de nuevo\n",
        "X_train_ohe_df = X_full_ohe.iloc[:n_train].copy()\n",
        "X_val_ohe_df   = X_full_ohe.iloc[n_train:].copy()\n",
        "\n",
        "# 3. Escalar las variables NUMÉRICAS (Igual que antes)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustamos solo con TRAIN\n",
        "X_train_ohe_df[num_cols] = scaler.fit_transform(X_train_ohe_df[num_cols])\n",
        "X_val_ohe_df[num_cols]   = scaler.transform(X_val_ohe_df[num_cols])\n",
        "\n",
        "# 4. Convertir a Tensores\n",
        "X_train_t = torch.tensor(X_train_ohe_df.values, dtype=torch.float32)\n",
        "X_val_t   = torch.tensor(X_val_ohe_df.values,   dtype=torch.float32)\n",
        "\n",
        "# Targets\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_val_t   = torch.tensor(y_val.values,   dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# 5. Dataloaders\n",
        "train_ds_ohe = TensorDataset(X_train_t, y_train_t)\n",
        "val_ds_ohe   = TensorDataset(X_val_t,   y_val_t)\n",
        "\n",
        "train_loader_ohe = DataLoader(train_ds_ohe, batch_size=256, shuffle=True)\n",
        "val_loader_ohe   = DataLoader(val_ds_ohe,   batch_size=256, shuffle=False)\n",
        "\n",
        "# Vemos cuánto creció la entrada\n",
        "input_dim_ohe = X_train_t.shape[1]\n",
        "print(f\"Nueva dimensión de entrada (Features): {input_dim_ohe}\")"
      ],
      "metadata": {
        "id": "_rKK6arlX90W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NoEmbeddingModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32], p_dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Mismas capas ocultas que el modelo anterior\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(p_dropout))\n",
        "            prev_dim = h\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1)) # Salida lineal (logit)\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "# Instanciamos el modelo con la nueva dimensión\n",
        "model_ohe = NoEmbeddingModel(input_dim=input_dim_ohe).to(device)\n",
        "print(model_ohe)"
      ],
      "metadata": {
        "id": "TOPCJX77YDQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weight = torch.tensor([3.0]).to(device)\n",
        "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer  = torch.optim.Adam(model_ohe.parameters(), lr=2e-4)\n",
        "\n",
        "n_epochs = 30\n",
        "\n",
        "history_ohe = {\n",
        "    \"train_loss\": [], \"val_loss\": [],\n",
        "    \"train_acc\": [],  \"val_acc\": [],\n",
        "    \"train_f1\": [],   \"val_f1\": []\n",
        "}\n",
        "\n",
        "print(\"Entrenando modelo SIN Embeddings (OHE)...\")\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model_ohe.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for x_batch, y_batch in train_loader_ohe:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device).view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model_ohe(x_batch)\n",
        "        loss   = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # --- Evaluación ---\n",
        "    model_ohe.eval()\n",
        "    with torch.no_grad():\n",
        "        # Train\n",
        "        logits_tr = model_ohe(X_train_t.to(device))\n",
        "        probs_tr  = torch.sigmoid(logits_tr).cpu().numpy()\n",
        "        preds_tr  = (probs_tr >= 0.5).astype(int)\n",
        "\n",
        "        # Val\n",
        "        logits_val = model_ohe(X_val_t.to(device))\n",
        "        probs_val  = torch.sigmoid(logits_val).cpu().numpy()\n",
        "        preds_val  = (probs_val >= 0.5).astype(int)\n",
        "\n",
        "        # Aseguramos que el target de validación tenga dimensión [N, 1]\n",
        "        loss_val = criterion(logits_val, y_val_t.to(device).view(-1, 1)).item()\n",
        "\n",
        "    # Métricas\n",
        "    acc_tr  = accuracy_score(y_train_np, preds_tr)\n",
        "    f1_tr   = f1_score(y_train_np, preds_tr, average='macro')\n",
        "    loss_tr = np.mean(train_losses)\n",
        "\n",
        "    acc_val = accuracy_score(y_val_np, preds_val)\n",
        "    f1_val  = f1_score(y_val_np, preds_val, average='macro')\n",
        "\n",
        "    history_ohe[\"train_loss\"].append(loss_tr)\n",
        "    history_ohe[\"val_loss\"].append(loss_val)\n",
        "    history_ohe[\"train_acc\"].append(acc_tr)\n",
        "    history_ohe[\"val_acc\"].append(acc_val)\n",
        "    history_ohe[\"train_f1\"].append(f1_tr)\n",
        "    history_ohe[\"val_f1\"].append(f1_val)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | loss: {loss_tr:.4f} - {loss_val:.4f} | acc: {acc_tr:.3f} - {acc_val:.3f} | f1: {f1_tr:.3f} - {f1_val:.3f}\")"
      ],
      "metadata": {
        "id": "d-7qOeR4YHe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Búsqueda del mejor umbral para OHE\n",
        "# Usamos 'probs_val' que quedó en memoria de la última época del entrenamiento OHE\n",
        "best_thr_ohe = 0.5\n",
        "best_f1_ohe = 0.0\n",
        "thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "for thr in thresholds:\n",
        "    preds = (probs_val >= thr).astype(int)\n",
        "    current_f1 = f1_score(y_val_np, preds, average='macro') # OJO: average='macro' para ser consistente\n",
        "\n",
        "    if current_f1 > best_f1_ohe:\n",
        "        best_f1_ohe = current_f1\n",
        "        best_thr_ohe = thr\n",
        "\n",
        "print(f\"Mejor F1 OHE encontrado: {best_f1_ohe:.4f} con umbral {best_thr_ohe:.2f}\")\n",
        "\n",
        "# 2. Aplicar el umbral ganador\n",
        "preds_val_opt = (probs_val >= best_thr_ohe).astype(int)\n",
        "\n",
        "# 3. Reportes Finales\n",
        "print(f\"\\n--- REPORTE FINAL OHE (Umbral {best_thr_ohe:.2f}) ---\")\n",
        "print(classification_report(y_val_np, preds_val_opt, digits=4))\n",
        "\n",
        "# 4. Matriz Normalizada Final\n",
        "print(\"Matriz de Confusión Normalizada (OHE Optimizado):\")\n",
        "cm_norm_ohe = confusion_matrix(y_val_np, preds_val_opt, normalize='true')\n",
        "\n",
        "# Graficamos\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm_ohe, display_labels=['<=50K', '>50K'])\n",
        "disp.plot(cmap='Reds', values_format='.2f', ax=ax)\n",
        "plt.title(f\"Matriz OHE Ajustada (Thr: {best_thr_ohe:.2f})\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QK060H5RZ7Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generamos el rango de épocas basado en los datos guardados\n",
        "epochs = range(1, len(history_ohe[\"train_acc\"]) + 1)\n",
        "\n",
        "# GRÁFICO 1: F1 MACRO\n",
        "plt.figure()\n",
        "plt.plot(epochs, history_ohe[\"train_f1\"], label=\"f1_train\")\n",
        "plt.plot(epochs, history_ohe[\"val_f1\"],   label=\"f1_val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 macro\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# GRÁFICO 2: ACCURACY\n",
        "plt.figure()\n",
        "plt.plot(epochs, history_ohe[\"train_acc\"], label=\"acc_train\")\n",
        "plt.plot(epochs, history_ohe[\"val_acc\"],   label=\"acc_val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "anvbmYPkZeLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## d) Conclusiones finales (2 puntos)\n",
        "\n",
        "- Elaborar una tabla comparativa con los resultados obtenidos por ambos modelos.\n",
        "- Redactar sus observaciones y apreciaciones derivadas de la comparación y plantear conclusiones fundamentadas respecto al desempeño de cada enfoque, justificando por qué uno funciona mejor o peor según las características del problema."
      ],
      "metadata": {
        "id": "URnyhcsZNGMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Definimos los datos basados en los resultados reales\n",
        "data = {\n",
        "    \"Métrica / Característica\": [\n",
        "        \"Umbral Óptimo (Threshold)\",\n",
        "        \"Aciertos Clase Mayoritaria (<=50K)\",\n",
        "        \"Aciertos Clase Minoritaria (>50K)\",\n",
        "        \"F1-Macro Final (Validación)\",\n",
        "        \"Dimensionalidad de Entrada\",\n",
        "        \"Estrategia de Representación\"\n",
        "    ],\n",
        "    \"Modelo con Embeddings\": [\n",
        "        \"0.65\",\n",
        "        \"87%\",\n",
        "        \"70%\",\n",
        "        \"~0.78\",\n",
        "        \"Baja (Vectores densos)\",\n",
        "        \"Semántica / Comprimida\"\n",
        "    ],\n",
        "    \"Modelo One-Hot Encoding\": [\n",
        "        \"0.70\",\n",
        "        \"90%\",\n",
        "        \"69%\",\n",
        "        \"~0.78\",\n",
        "        \"Alta (Vectores dispersos)\",\n",
        "        \"Dispersa / Explícita\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_comparacion = pd.DataFrame(data)\n",
        "df_comparacion"
      ],
      "metadata": {
        "id": "40zK63kieSq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al analizar los resultados de ambos enfoques, noté que no hay una diferencia drástica en el rendimiento final medido por el F1-Macro. Ambos modelos, utilizando la misma arquitectura de red neuronal, lograron resolver el problema de manera satisfactoria, en gran parte gracias al uso de los pesos en la función de costo y al ajuste manual de los umbrales que realicé.\n",
        "\n",
        "En cuanto a las diferencias de comportamiento, el modelo con One-Hot Encoding resultó ser un poco más conservador y preciso clasificando la clase mayoritaria, alcanzando un 90% frente al 87% del modelo de embeddings.\n",
        "\n",
        "Por otro lado, el modelo de Embeddings tuvo una ligera ventaja en la sensibilidad de la clase minoritaria (70% frente a 69%). Esto sugiere que la representación densa y continua ayudó al modelo a generalizar mejor y capturar relaciones ocultas entre las categorías, como agrupar niveles educativos similares, lo que facilitó la detección de los casos de ingresos altos.\n",
        "\n",
        "Creo que la razón por la que el One-Hot Encoding funcionó casi tan bien como los Embeddings radica en la naturaleza del dataset. La variable con más cardinalidad es el país de origen con 41 categorías. Aunque es un número alto, sigue siendo manejable para una red neuronal moderna y no sufrimos severamente la maldición de la dimensionalidad. Además, con unos 30,000 registros, hay datos suficientes para que el modelo aprenda los pesos de esas columnas extra sin sobreajustarse inmediatamente.\n",
        "\n",
        "Como conclusión final, para ESTE problema en particular, el OHE fue igual de bueno (o hasta mejor en precisión) y más simple. Pero el Embedding es la solución correcta si pensamos en escalar a problemas más grandes."
      ],
      "metadata": {
        "id": "Z4GFPbuqbAVC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DdvvoMCbACw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}